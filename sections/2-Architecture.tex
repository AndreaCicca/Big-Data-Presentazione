\section{Architecture}

%------------------------------------------------
\begin{frame}
	\frametitle{Optimistic concurrency control P1}
 In a system with \textbf{optimistic concurrency control}, transactions operate under the optimistic assumption that conflicts are rare. 
\vspace{0.5cm}

 OCC assumes that multiple transactions can frequently complete without interfering with each other.
\end{frame}
%------------------------------------------------

\begin{frame}
	\frametitle{Optimistic concurrency control P2}
While running, transactions use data resources without acquiring locks on those resources. Before committing, each transaction verifies that no other transaction has modified the data it has read. 
\vspace{0.2cm}

\begin{center}
    \includegraphics[width=0.5\textwidth]{img/2-Architecture/Optimistic Concurrency Control.png}
\end{center}

If the check reveals conflicting modifications, the committing transaction rolls back and can be restarted.


\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Multiversion Concurrency Control (MVCC)}
  FoundationDB utilizes \textbf{multiversion concurrency control} as part of its optimistic approach. With MVCC, each transaction operates on a "version" of the data, reflecting a database state at a certain point in time (always a consistent view when reading).
  \vspace{0.5cm}
  
  Transaction can only be performed within the MVCC window (5 seconds).
\end{frame}
%------------------------------------------------


\begin{frame}
    \frametitle{Architecture and Transaction Processing}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item \textbf{Control Plane}
                \item \textbf{Data Plane}
                \begin{itemize}
                    \item Transaction System (TS)
                    \item Log System (LS)
                    \item Storage System (SS)
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}


%------------------------------------------------

\begin{frame}
	\frametitle{Control plane P1}

The control plane is responsible for persisting critical metadata of the cluster for high availability.
\textbf{Cluster Controller} monitors all servers in the cluster and recruits 3 processes:
\begin{itemize}
    \item Sequencer (Data Plane)
    \item Data Distributor
    \item Ratekeeper
\end{itemize}

Coordinators form a Paxos group (consensus protocol) and elect the Cluster Controller.
\end{frame}

%------------------------------------------------


\begin{frame}
    \frametitle{Control plane P2}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
        \item \textbf{Data Distributor} is responsible for monitoring failures and balancing data among Storage Servers.
        \item \textbf{Ratekeeper} provides overload protection for the cluster.
        \end{itemize}
        
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Control plane.png}
        \end{column}
    \end{columns}
\end{frame}



%------------------------------------------------


\begin{frame}
    \frametitle{Data plane: Transaction system (TS) P1}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
    \item \textbf{Sequencer}: assigns a read and a commit version to each transaction
    \item \textbf{Proxies}: offer MVCC read versions to clients and orchestrate transaction commits
    \item \textbf{Resolvers}: check for conflicts among transactions (5 seconds of history)
   \end{itemize}
        
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Transaction System.png}
        \end{column}
    \end{columns}
\end{frame}

%------------------------------------------------

\begin{frame}
	\frametitle{Data plane: Transaction system (TS) P2}

FoundationDB uses \textbf{multiversion concurrency control} to provide transactionally isolated reads without locking data or blocking writes.
\vspace{0.5cm}

\textbf{Optimistic concurrency control} ensures that deadlocks are impossible and that slow or failing clients cannot interfere with the operation of the database.
	
\end{frame}

% %------------------------------------------------


\begin{frame}
    \frametitle{Data plane: Log system (LS)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \textbf{Log Servers} act as replicated, distributed persistent queues, each queue storing "Write-Ahead Log" data for a Storage Server.
        
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/log system.png}
        \end{column}
    \end{columns}
\end{frame}

% %------------------------------------------------


\begin{frame}
    \frametitle{Data plane: Storage system (SS)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        The Storage system consists of a number of \textbf{Storage Servers}, each
storing a set of data shards (contiguous key ranges),
and serving client reads. 
\vspace{0.5cm}

Storage Servers are the majority of processes in the system, and together they form a distributed B-tree.
The vast majority of processes in a cluster are storage servers.
        
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Storage system.png}
        \end{column}
    \end{columns}
\end{frame}

% %------------------------------------------------
\begin{frame}
	\frametitle{Scaling}
Scaling is just adding processes for each role. \\

\begin{itemize}
\item Clients read from sharded Storage Servers, so reads scale
linearly with the number of Storage Servers. 
\end{itemize}

\vspace{0.5cm}
Coordinators and control plane's singleton processes like \textbf{Cluster Controller} and \textbf{Sequencer} only perform limited metadata operations.
	
\end{frame}
% %------------------------------------------------

\begin{frame}
    \frametitle{Transactions: Read}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
    \item Client $\rightarrow$ Proxies to obtain a read version (timestamp).
    \item Proxy $\rightarrow$ the Sequencer requesting a read version, that is  least as large as all previously issued transaction commit versions.
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}

% %------------------------------------------------

\begin{frame}
    \frametitle{Transactions: Read P2}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
    \item Proxy sends read version to client.
    \item Client reads from Storage Servers using the read version issued by the Proxy.
    \item Client can cache this information for the next read if needed.
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}

% %------------------------------------------------

\begin{frame}
    \frametitle{Transactions: Writes}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
                    \item When the client decides to commit the transaction, it sends the transaction data to one of the Proxies (read and write that you have done in the transaction)
                \item Client waits for a response from the Proxy (commit or abort)
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}


% %------------------------------------------------
\begin{frame}
    \frametitle{Transactions: Proxy commits}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
\item Transaction is forwarded to Resolvers that have to handle the optimistic concurrency control by checking for read-write conflicts.
    \item Transaction is forwarded to a set of designated Log Servers
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}
% %------------------------------------------------
\begin{frame}
    \frametitle{Transactions: Proxy commits P2}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{enumerate}
    \item Proxy reports the committed version to the Sequencer, ensures that later transactions' read versions occur after this commit
    \item Proxy replies to the client
            \end{enumerate}
        \end{column}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/2-Architecture/Architecture and transaction processing.png}
        \end{column}
    \end{columns}
\end{frame}
% %------------------------------------------------

\begin{frame}
	\frametitle{Transactions P3}

 FoundationDB offers
 
 \begin{itemize}
     \item \textbf{Read-Only Transactions/Snapshot Reads}: They occur at a specific read version, ensuring serializability. Consistent view of the database at a specific point in time (no conflict in transactions)
 \end{itemize}

\end{frame}


% %------------------------------------------------


\begin{frame}
	\frametitle{Strict serializability}
\begin{itemize}
  \item FDB ensures strict serializability via Serializable Snapshot Isolation (SSI), combining OCC with MVCC.
%   \item Transactions obtain read and commit versions from the Sequencer.
  \item A transaction can commit only when all Resolvers admit the transaction
  \item The key space is divided among Resolvers for parallel conflict detection.
  \item Optimistic Concurrency Control design simplifies TS and SS interactions but results in wasted work for aborted transactions (client restart the transaction )
\end{itemize}

 \end{frame}

%------------------------------------------------

% da fare

% \begin{frame}
% 	\frametitle{Logging protocol ***}
%     Test

% \end{frame}

% %------------------------------------------------


\begin{frame}
	\frametitle{Transaction system recover}
    \begin{itemize}
    
        \item Traditional databese uses ARIES Recovery (checkpoint, redo, undo)
        \item In FD the recovery is purposely made very cheap, greatly simplifying design choice: redo log processing is the same as the normal log forward path
        
\end{itemize}
\end{frame}

%------------------------------------------------
% \begin{frame}
% 	\frametitle{After detecting a failure}
%     \begin{itemize}
%         \item The recovery process starts by
%         detecting a failure and recruiting a new transaction system
%         \item CC (Cluster Controller) reads the previous TS configuration from Coordinators and locks this information
%         to prevent another concurrent recovery
%         \item CC recovers previous TS system states, including information about older Log Servers
%         \item CC recruits a new set of Sequencer, Proxies, Resolvers,
%         and Log Servers
%         \item need to ensure all transactions in the LS are durable and retrievable by Storage Servers for recovery

%     \end{itemize}

% \end{frame}



%------------------------------------------------


\begin{frame}
	\frametitle{Replication: Metadata}
    \begin{itemize}
      \item System metadata of the control plane is stored on Coordinators using \textbf{Disk Paxos} (consensus protocol used for reliable data replication across multiple nodes optimized for disk storage)
      \item As long as a quorum (majority) of Coordinators are live, this metadata can be recovered.

\end{itemize}
\end{frame}

 %------------------------------------------------


\begin{frame}
	\frametitle{Replication: Log}
\begin{itemize}

      \item When a Proxy writes logs to Log Servers, each sharded log record is synchronously replicated on \textbf{multiple Log Servers}.
      \item Only when all Log Servers have replied with successful persistence can the Proxy send back the commit response to the client.
      \item Failure of a Log Server results in \textbf{transaction system recovery}.

\end{itemize}
\end{frame}

 %------------------------------------------------


\begin{frame}
	\frametitle{Replication: Storage}
\begin{itemize}

      \item Every shard (key range) is asynchronously replicated with multiple Storage Servers, forming a "\textbf{team}".
      \item A Storage Server usually hosts multiple shards (key range) to evenly \textbf{distribute data across many teams}.
      \item Failure of a Storage Server triggers Data Distributor to move data from affected teams to healthy ones.

\end{itemize}
\end{frame}